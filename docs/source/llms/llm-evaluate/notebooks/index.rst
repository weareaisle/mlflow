LLM Evaluation Examples
=======================

The notebooks listed below contain step-by-step tutorials on how to use MLflow to evaluate LLMs. 
The first notebook is centered around evaluating an LLM for question-answering with a 
prompt engineering approach. The second notebook is centered around evaluating a RAG system. 
Both notebooks will demonstrate how to use MLflow's builtin metrics such as token_count and 
toxicity as well as LLM-judged intelligent metrics such as answer_relevance. 

.. toctree::
    :maxdepth: 1
    :hidden:

    question-answering-evaluation.ipynb
    rag-evaluation.ipynb

QA Evaluation Notebook
------------------------------------

If you would like a copy of this notebook to execute in your environment, download the notebook here:

.. raw:: html

    <a href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/llm-evaluate/notebooks/question-answering-evaluation.ipynb" class="notebook-download-btn">Download the notebook</a><br/>

To follow along and see the sections of the notebook guide, click below:

.. raw:: html

    <a href="question-answering-evaluation.html" class="download-btn">View the Notebook</a><br/>


RAG Evaluation Notebook
------------------------------------

If you would like a copy of this notebook to execute in your environment, download the notebook here:

.. raw:: html

    <a href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/llm-evaluate/notebooks/rag-evaluation.ipynb" class="notebook-download-btn">Download the notebook</a><br/>

To follow along and see the sections of the notebook guide, click below:

.. raw:: html

    <a href="rag-evaluation.html" class="download-btn">View the Notebook</a><br/>


