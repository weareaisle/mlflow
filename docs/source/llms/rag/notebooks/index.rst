============================================
Question Generation for Retrieval Evaluation
============================================

This notebook is a step-by-step tutorial on how to generate a question dataset with 
LLMs for retrieval evaluation within RAG. It will guide you through getting a document dataset,
generating diverse and relevant questions through prompt engineering on LLMs, and analyzing the 
question dataset. The question dataset can then be used for the subsequent task of evaluating the 
retriever model, which is a part of RAG that collects and ranks relevant document chunks based on
the user's question.

.. toctree::
    :maxdepth: 1
    :hidden:

    question-generation-retrieval-evaluation.ipynb

Question Generation for RAG Notebook
------------------------------------

If you would like a copy of this notebook to execute in your environment, download the notebook here:

.. raw:: html

    <a href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb" class="notebook-download-btn">Download the notebook</a><br/>

To follow along and see the sections of the notebook guide, click below:

.. raw:: html

    <a href="question-generation-retrieval-evaluation.html" class="download-btn">View the Notebook</a><br/>
